{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Performance of ResNet152 with Confusion Matrix and ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from keras example cifar10_cnn.py\n",
    "Train ResNet-18 on the CIFAR10 small images dataset.\n",
    "\n",
    "GPU run command with Theano backend\n",
    "(with TensorFlow, the GPU is automatically used):\n",
    "\n",
    "THEANO_FLAGS=mode=FAST_RUN,device=gpu,floatX=float32 python cifar10.py\n",
    "\n",
    "\"\"\"\n",
    "from __future__ import print_function\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "import data_prepare as dp\n",
    "from keras.models import model_from_json\n",
    "\n",
    "import numpy as np\n",
    "import resnet\n",
    "import time\n",
    "\n",
    "\n",
    "lr_reducer = ReduceLROnPlateau(factor=np.sqrt(0.1), cooldown=1, patience=5, min_lr=0.5e-6)\n",
    "early_stopper = EarlyStopping(min_delta=0.001, patience=10)\n",
    "csv_logger = CSVLogger('resnet152_'+time.strftime(\"%Y%m%d_%H%M%S\")+'.csv')\n",
    "\n",
    "batch_size = 16 \n",
    "nb_classes = 2\n",
    "nb_epoch = 100\n",
    "data_augmentation = True\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 224, 224\n",
    "# The CIFAR10 images are RGB.\n",
    "img_channels = 1\n",
    "\n",
    "def train():\n",
    "    # The data, shuffled and split between train and test sets:\n",
    "    (X_train, y_train), (X_test, y_test) = dp.load_Data()\n",
    "\n",
    "    # Convert class vectors to binary class matrices.\n",
    "    Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "    Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "\n",
    "    # subtract mean and normalize\n",
    "    mean_image = np.mean(X_train, axis=0)\n",
    "    X_train -= mean_image\n",
    "    X_test -= mean_image\n",
    "    X_train /= 128.\n",
    "    X_test /= 128.\n",
    "    model = resnet.ResnetBuilder.build_resnet_152((img_channels, img_rows, img_cols), nb_classes)\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    #model.load_weights('resnet152_weights_tf.h5')\n",
    "\n",
    "    if not data_augmentation:\n",
    "        print('Not using data augmentation.')\n",
    "        model.fit(X_train, Y_train,\n",
    "                  batch_size=batch_size,\n",
    "                  nb_epoch=nb_epoch,\n",
    "                  validation_data=(X_test, Y_test),\n",
    "                  shuffle=True,\n",
    "                  callbacks=[lr_reducer, csv_logger]) #, early_stopper\n",
    "    else:\n",
    "        print('Using real-time data augmentation.')\n",
    "        # This will do preprocessing and realtime data augmentation:\n",
    "        datagen = ImageDataGenerator(\n",
    "            featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "            samplewise_center=False,  # set each sample mean to 0\n",
    "            featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "            samplewise_std_normalization=False,  # divide each input by its std\n",
    "            zca_whitening=False,  # apply ZCA whitening\n",
    "            rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=True,  # randomly flip images\n",
    "            vertical_flip=True)  # randomly flip images\n",
    "\n",
    "        # Compute quantities required for featurewise normalization\n",
    "        # (std, mean, and principal components if ZCA whitening is applied).\n",
    "        datagen.fit(X_train)\n",
    "\n",
    "        # Fit the model on the batches generated by datagen.flow().\n",
    "        model.fit_generator(datagen.flow(X_train, Y_train, batch_size=batch_size),\n",
    "                            steps_per_epoch=X_train.shape[0] // batch_size,\n",
    "                            validation_data=(X_test, Y_test),\n",
    "                            epochs=nb_epoch, verbose=1, max_q_size=100,\n",
    "                            callbacks=[lr_reducer, csv_logger]) #, early_stopper\n",
    "        # predictions\n",
    "        # preds = model.predict(X_test)\n",
    "        # print('Predicted:', preds[0])\n",
    "        \n",
    "        # serialize model to JSON\n",
    "        modelName = \"model152_\" + time.strftime(\"%Y%m%d_%H%M%S\") + \".json\"\n",
    "        model_json = model.to_json()\n",
    "        with open(modelName, \"w\") as json_file:\n",
    "            json_file.write(model_json)\n",
    "        print(\"Saved model to disk\")\n",
    "\n",
    "        # serialize weights to HDF5\n",
    "        modelWeight = \"model152Weight_\" + time.strftime(\"%Y%m%d_%H%M%S\") + \".h5\"\n",
    "        model.save_weights(modelWeight)\n",
    "        print(\"Saved model weight to disk\")\n",
    "        \n",
    "        return model\n",
    "\n",
    "def predict():\n",
    "    # load json and create model\n",
    "    json_file = open('model.json', 'r')\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    \n",
    "    # load weights into new model\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    print(\"Loaded model from disk\")\n",
    "\n",
    "# model = train()\n",
    "# predict()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:BleedingSites] *",
   "language": "python",
   "name": "conda-env-BleedingSites-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
